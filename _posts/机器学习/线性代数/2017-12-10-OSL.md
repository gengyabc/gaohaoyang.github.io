---
layout: post
title:  "线性代数 16 - 投影矩阵和最小二乘"
categories: 线性代数
tags:  数学
mathjax: true
author: Geng
---

* content
{:toc}

我们求解 $Ax=b$ 的时候, 经常发现无解. 一般来说就是因为**方程太多了, 比未知数还多**. 用矩阵的术语来说, 那么就是 $m \times n$ 矩阵 $A$, $m > n$, $A$ 的 $n$ 列只能生成 $m$ 维空间的一个子空间. 除非特别巧, 否则 $b$ 肯定在 $A$ 的列空间之外. 消元法肯定不能解决这个问题, 那怎么办呢?





> 为什么我要求解一个不可解的方程组呢? 在[](14. 正交向量与子空间) 部分有过介绍. 简单来说就是我们不能因为测量有误差就不解方程了!

假设误差是 $e$, 那么 $e=b-Ax$. 如果测量没有误差, 那么我们可以求解出 $Ax=b$. 不过没有测量误差不可能的. 那么我们可以使用**最小二乘法**求出一个最优解 $\hat{x}$, 它使得 $e$ 的长度最小.

上一部分我们着重考虑 $p$, 现在要考虑 $\hat{x}$ 了. 我们已经有了 $p=A\hat{x}$, 这部分就要求解 $A^TA\hat{x} = A^Tb$

## 投影矩阵回忆

考虑这两个例子:

1. 如果 $b\in C(A)$，$Pb=?$
2. 如果 $b\bot C(A)$，$Pb=?$

> 不要想公式, 不要去理解, 去看!

第一个例子. $b$ 就在 $A$ 的列空间内. 就像一条画在纸面的直线, 它的投影就是自己.

第二个例子. $b$ 与 $A$ 的列空间垂直. 就像一支垂直放在桌面的笔, 这个笔无限的细, 投影只能是 $0$.

## 拟合 $m$ 个点

最小二乘法的一个经典应用场合就是用一条直线拟合 $m$ 个点 $(t_1, b_1), \cdots (t_m, b_m)$. 考虑拟合 $(0, 6),\ (1, 0), \ (2, 0)$ 三个点. 很明显, 没有直线 $b=C + Dt$ 能同时穿过这三个点.

$$
\begin{equation}
\begin{cases}
C+D \cdot 0&=6 \\
C+D\cdot 1&=0 \\
C+D\cdot 2&=0 
\end{cases}
\end{equation}
$$

使用 $A^TA\hat{x} = A^Tb$ 可以求出这个最优解. 这里通过最小误差来考虑.

## 最小化误差

怎样才能使 $e=b-Ax$ 尽量小呢?

首先明确: 

$$
A=
\begin{bmatrix}
1&0 \\ 
1&1 \\
1&1
\end{bmatrix}
, \ \ 
x=
\begin{bmatrix}
C \\ 
D
\end{bmatrix}
, \ \ 
b=
\begin{bmatrix}
6 \\ 
0 \\
0
\end{bmatrix}
$$

### 几何图像

* 列图像: 每一个 $Ax$ 都在 $(1, 1, 1)$ 和 $(0, 1, 2)$ 组成的列空间. 在这个平面上, 我们要寻找离 $b$ 最近的点, 也就是 $p$. 满足 $A \hat{x}$ 最好的选择就是 $p$. 最小的误差就是 $e=b-Ax$. 因为 $Ax$ 就是对 $A$ 的两个列向量的线性组合, 所以 $p$ 一定在列空间(平面)上
* 行图像: 现在有两个未知系数 $C, D$ 待求, 也只有两个变量 $(t_i, b_i)$, 在 $(t_i, b_i)$ 构成的直角坐标系中, 满足方程组 (1) 的图像是一条直线(有解的话). 同时结合列图像, 向量 $t$ 和 $b$ 都在 $A$ 的列空间. 如果一个点 $(t_i, b_i)$ 在列空间, 那么这个点就是在方程组 (1) 规定的直线上(有的话). 因为 $p$ 在 $A$ 的列空间, 所以三个高度为 $(p_1, p_2, p_3)$ 的点在一条线上.

![]({{ site.url }}/assets/images/posts/machineLearning/线性代数/2017-12-10-OSL/geo.jpg)

### 线性代数理解

每一个向量 $b$ 都可以分解成两部分: 位于列空间的 $p$, 还有垂直于 $N(A^T)$ 的 $e$. $Ax=b=p+e$ 虽然无解, 但是 $A\hat{x}=p$ 有解. 这个解使得 $e$ 最小.

$b$ 的三个分量上都减去某个误差 $e$，使得三点能够共线，同时使得 $e_1^2+e_2^2+e_3^2$ 最小，找到拥有最小平方和的解（即最小二乘），即 $\left\|Ax-b\right\|^2=\left\|e\right\|^2$ 最小.此时向量 $b$ 变为向量$p=(p_1, p_2, p_3)$. 我们现在做的运算也称作线性回归（linear regression），使用误差的平方和作为测量总误差的标准.

![]({{ site.url }}/assets/images/posts/machineLearning/线性代数/2017-12-10-OSL/alg.jpg)


### 微积分

使用 $A^TA\hat{x} = A^Tb$ 可以求出这个最优解. 这里通过最小误差来考虑: $e_1^2+e_2^2+e_3^2$ 最小.

 就是:$
(d + D \cdot 0 - 6)^2 + (C + D \cdot 1)^2 + (C + D \cdot 2)^2
$ 最小. 分别对两个未知数求偏导, 可以解出 $C , \ D$, 可以证明, 这个求解过程最后就是求解 $A^TA\hat{x} = A^Tb$.

## 总体图像

![]({{ site.url }}/assets/images/posts/machineLearning/线性代数/2017-12-10-OSL/big.png)

这个场景中, $Ax=b$ 无解.

不同于[14. 正交向量与子空间]()的第一幅配图($Ax=b$有解), 这里我们不是分割 $x$, 而是分割 $b=p+e$, 求解 $A\hat{x}=p$.

> 注意 $N(A)$ 很小, 只有一个点. 因为列向量线性无关的话, $Ax=0$ 的解只有 $x=0$. 



