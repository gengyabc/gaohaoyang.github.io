---
layout: post
title:  "概率论 7. 伯努利过程"
categories: 概率论
tags:  数学
author: Geng
---

* content
{:toc}


## 简介
伯努利过程可视为独立投掷硬币的序列, 每次正面概率为*p*, *0 < p < 1*. 伯努利过程由一串伯努利试验组成. 伯努利过程经常用来对下面这些系统建模: 顾客到来, 服务器收到一个请求. 时间常常被离散化为若干时间段, 在一个时间段中, 只要有顾客到达, 就认为这个时间段试验成功. 因此, 我们常常使用**到达**而不是成功.

> MIT的教授讲到伯努利抛硬币的时候, 讲了一个小笑话. 伯努利搞了一堆公式定理, 但是伯努利不是一个人, 而是[伯努利家族](http://baike.baidu.com/item/%E4%BC%AF%E5%8A%AA%E5%88%A9), 可以想象一群伯努利在饭桌上掷骰子玩.






人们常常对一定时间内到达的次数, 或者首次到达时间, 或者第*k*次到达时间感兴趣. 

### *n*个时间段内*k*个时间段成功的分布
显然, 这就是 *n* 选 *k* 的问题, 服从二项分布\\(S \sim (B(n, p)\\):

$$ p_S(k) = P(S = k) = \left( \begin{matrix} n\\ k\end{matrix} \right)p^{k}(1-p)^{n-k}, \ k = 0, 1, ..., n $$

其中\\(E[S] = np\\), 这个期望很好理解, 比如掷骰子的成功率是 *1/10* , 那么试验 *10* 次差不多应该成功一次, 试验 *n* 次的话就应该成功 * n/10* 次了. 

### 首次成功时刻T的分布
显然, 这个是几何分布:

$$ p_T(t) = (1-p)^{t-1}p, \ t = 1, 2... $$

其中\\(E[T] = 1/p\\), 这个期望也很好理解, 比如掷骰子的成功率是 *1/10* , 刚刚成功了一次, 那么下一次成功要到什么时候呢? 可以想到试验 *10* 次差不多应该成功一次, 那么下次成功应该就是 *10* 次了. 

### 第k次到达时间的分布
稍后介绍

## 独立性和无记忆性
每次试验相互独立, 隐含了一个重要特征: 无记忆性. 过去发生的任何事情都不能对未来产生影响. 比如连续抛硬币, 连续100个正面并不能告诉你任何第101次是正还是反的信息, 101不知道1 ~ 100发生过什么. 

那么我们可以从上面叙述中看出, 不管试验从什么时候开始计算, 我们使用的模型都是相同的伯努利模型, 这就是伯努利过程的一个重要性质: **全新开始**.

## 相邻到达的间隔时间
第*k*次到达的时间是一个随机变量, 记为\\(Y_{k}\\), 与其相关的是第*k*次相邻到达的间隔时间, 记为\\(T_{k}\\), 如下图所示:

![]({{ site.url }}/assets/images/posts/machineLearning/概率论/2017-06-13-bernoulli-process/k_interval.png)

从上图可以容易看出:

$$ Y_k = T_1 + T_2 + ... + T_k $$

可见, 伯努利过程可以看成是若干个几何分布的序列, 且其达到时间为: \\(T_1\\), \\(T_1 + T_2\\), \\(T_1 + T_2 + T_3\\)等.

## 第k次到达时间
根据上面讨论, 第k次到达时间\\(Y_k\\)就是: \\(Y_k = T_1 + T_2+ ... + T_k\\).

*k* 次之前(\\(t - 1\\)时刻前), 已经有\\(k - 1\\)次到达, 而且这\\(k - 1\\)次随机分布在\\(t - 1\\)个时间段. 显然这\\(k - 1\\)次构成一个二项分布. 我们又知道, 在 *t* 时刻(第 *t* 次试验), 发生了第 *k* 成功. 

那么第k次到达时间t的分布可写为:

$$ p_{Y_k}(t) = ( \begin{matrix} t-1\\ k-1\end{matrix} )p^{k}(1-p)^{t-k}, \ t = k, k+1, ...$$ 

这个分布叫做**k阶帕斯卡分布**.

其中\\(E[Y-k] = k/p\\), 有了上面讨论的首次到达时间, 考虑到这 *k* 次试验相互独立, 而且每次试验都需要差不多 *1/p* 这么多时间, 那么 *k* 次这种试验的时间就是 *k/p* 了吧. 

## 伯努利过程的分裂与合并
抓住独立性这个关键, 这个问题so easy.

## 二项分布的泊松近似
我们知道二项分布\\((B(n, p)\\)期望为\\(np\\). 我们考虑一种极端情况: 保持*np*不变, 使劲缩小*n*, 同时使劲增大*p*. 比如世界上一天发生空难的次数, 或者一本书错别字的数目. 我们记 \\(np = \lambda\\):

$$ p_Z(k) = e^{-\lambda} \frac {\lambda^{k}} {k!}, \ k = 0, 1, 2 ... $$

上面这个分布是泊松分布, 是一种二项分布在\\(n\\)很大而\\(p\\)很小的情况的近似.
