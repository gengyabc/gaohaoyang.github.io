---
layout: post
title:  "概率论 3. 离散随机变量"
categories: 概率论
tags:  数学
author: Geng
---

* content
{:toc}


前面已经介绍过随机变量的概念，这里先从离散随机变量说起。

## 什么是离散随机变量
可能取值是有限多个或者**可列**无限多个。

可数好理解，什么是**可列**无限多个呢？比如说所有整数是可以列出的，但是是无限多个的。






### 分布律（probability mass function， PMF）
分布律的字面翻译就是概率质量函数。

设离散型随机变量*X*，发生事件\\(\\{X = x_k\\} \\)的概率可以记为：

$$ p_X(x) = P\{X = x_k\}  $$

大写字母*X*代表随机变量，小写字母*x*代表代表实数值，也就是随机变量*X*在实数轴上的映射。

我们可以将离散随机变量看成是空间中一块一块的体积块，每个体积块都有一定的质量，分布律（概率质量函数）就是这些体积块在空间的分布规律。

## 几何分布
假设我们抛硬币，正面概率是*p*，连续抛直到看到正面。那么需要抛*k*次的概率是多少？经过前面的介绍，这个分布律应该很容易：

$$ p_x(k) = (1-p)^{k-1}p $$

上面就是几何分布了。这其实就是做重复*k*次独立试验，直到成功为止。

这里，我先开个小差，说下**几何分布为什么叫几何分布**。


### 看图说话

$$ p_x(k) = (1-p)^{k-1}p $$

先看上面公式，随着*k*的不同，形成的数列是一个**等比数列**。看下图有个直观感觉：

![]({{ site.url }}/assets/images/posts/machineLearning/概率论/2017-05-21-permulation-combination/geo.png)

### 几何数列
等比数列还有一个名字，叫做**几何数列**。

那么为什么也叫**几何数列**呢？因为每项都是前后两项的**几何平均数**。

那么问题又来了，什么是**几何平均数**呢？

### 几何平均数
是指n个观察值连乘积的n次方根:

![](https://wikimedia.org/api/rest_v1/media/math/render/svg/026cae6801f672b9858d55935ec7397183dc3a36)

看到这个公式可能有点懵，但是换一种说法你可能立刻有了感觉，虽然其实你可能一直不知道这是在说啥：**方圆八百里**。

方圆八百里？到底是多大？我记得语文老师说过古人计算矩形面积，算法是对一个矩形削长补短，然后计算整合出来的这个相同面积的正方形的边长是多少，这个边长就是你可能经常听说的“方圆八百里”的“八百”。

了解这个**方圆**概念，注意到这就是在做几何平均了吗？一个矩形边长a和b，面积是ab，以古人的算法，乃为方圆\\(\sqrt {ab} \\)。这不就是**几何平均**吗？有没有突然感觉古人好nb。

至此，你知道为什么叫**几何分布**了吧？

## 伯努利试验
伯努利试验只有两种可能结果。独立重复进行n次，就是n重伯努利试验。

简单的，抛一次硬币就是一个伯努利试验。

## 二项分布
假设抛硬币*n*次，每次正面概率为*p*，那么反面概率是*p-1*，每次试验相互独立。那么有k次正面的概率分布就是**二项分布**。

$$ p_X(k) = P(X = k) = \left( \begin{matrix} n\\ k\end{matrix} \right)p^{k}(1-p)^{n-k} $$

当*n*足够大时，二项分布会近似为**正态分布**，如下图*n* = 6、*p* = 0.5时的二项分布以及正态近似：

![](https://upload.wikimedia.org/wikipedia/commons/thumb/b/b7/Binomial_Distribution.svg/375px-Binomial_Distribution.svg.png)

## 随机变量的函数
已知一个随机变量*X*，我们也可以将其转换从而生成另一个随机变量。比如说单位转换。

我们可以将这个转换关系写为一个函数：

$$ Y = g(X)  $$

可以将这个过程看成一个空间变换的过程，即*X*空间变到了*Y*空间。

例如\\(p_X(x)\\)在[-4, 4]范围概率是*1/9*，那么设 \\(y = \|x\|\\)，求\\(p_Y(y)\\)。我们可以根据下图来分析：

![]({{ site.url }}/assets/images/posts/machineLearning/概率论/2017-05-22-discrete/transform.png)

可见*y*取值在[0, 4]，而且这个范围内除了*0*，其他取值概率都是*2/9*。

上面过程可以归纳为：Y空间某事件y发生的概率，等于y在X空间对应的所有x发生的概率的和，即：

$$ p_Y(y) = \sum _{\{ x | g(x) = y \} }P_X \left( x\right)  $$

## 期望
期望就是平均数。对应概率质量函数的语境，我们可以把它理解为**重心**。那么，x就是重量为\\(P_X (x)\\)这个质量块（或者空间中的体积块）的坐标位置。

$$ E[X] =\sum_{x}xP_X (x) $$

### 随机变量的函数的期望
假设\\( Y = g(X) \\)，那么我们可以老老实实的计算它的期望：

$$ E[Y] =\sum_{y}yP_Y (y) $$

也可以这样来偷懒：

$$ E[Y] =\sum_{x}g(x)P_X (x) $$

这是为什么呢？我们可以用重心的概念来理解，如下图所示：

![]({{ site.url }}/assets/images/posts/machineLearning/概率论/2017-05-22-discrete/center.png)

X空间通过g(x)变化到Y空间，各个质量块根据g(x)进行了重新分布，形成了新的质量块（上图中摞在一起的质量块看成Y空间的一个质量块），但是不管怎么分布，X空间对应过来的每一个旧质量块的质量\\(P_X (x)\\)是不变的。在Y空间相对在X空间，变化的仅仅是每个旧质量块的位置坐标，变为了g(x)。这样，如果我们还以X空间的旧质量块考虑问题，Y空间中心的问题就变成了新的位置和旧的质量块的乘积的和，也就是上面的公式。

### 期望的性质
* \\(E[C] = C\\)

一个点的中心，当然还是自己。

![]({{ site.url }}/assets/images/posts/machineLearning/概率论/2017-05-22-discrete/cons.png)

* \\( E[CX] = CE[X] \\)

放大之后，重心自然有一个等比例的偏移

![]({{ site.url }}/assets/images/posts/machineLearning/概率论/2017-05-22-discrete/scale.png)

* \\( E[X+b] = E[X] + b \\)

移动位置后，重心肯定会相应的偏移

![]({{ site.url }}/assets/images/posts/machineLearning/概率论/2017-05-22-discrete/move.png)

## 方差
方差用来描述变量偏离平均值的程度：

$$ var(X) = E[(X-E(X))^2] = E[X^2] - (E[X])^2 $$

方差比较难以建立起空间上的直觉，这部分我还要研究一下。

## 标准差
标准差就是方差的平方根

## 协方差
我们先来看看它的公式：

$$ cov(X,Y) = E[(X-E(X))(Y-E(Y))] = E[XY]-E[X]E[Y] $$

可以想象，如果两个变量的变化趋势一致，也就是说如果其中一个大于自身的期望值，另外一个也大于自身的期望值，那么两个变量之间的协方差就是正值。如果两个变量的变化趋势相反，即其中一个大于自身的期望值，另外一个却小于自身的期望值，那么两个变量之间的协方差就是负值。

根据上面公式，如果X与Y是独立的，那么二者之间的协方差就是0。但是，如果X与Y的协方差为0，二者并不一定是独立的。

不过协方差不是很好比较，所以我们引入相关的概念。

## 相关系数
相关系数可以看做是归一化的协方差。

$$ \rho = \dfrac {cov(X,Y)} {\sqrt {var(X)var(Y})} $$

相关系数取值在[-1, 1]，系数的值为1意味着X和Y可以很好的由直线方程来描述，所有的数据点都很好的落在一条直线上，且Y随着X的增加而增加。系数的值为−1意味着所有的数据点都落在直线上，且Y随着X的增加而减少。系数的值为0意味着两个变量之间没有线性关系。

![](https://upload.wikimedia.org/wikipedia/commons/thumb/d/d4/Correlation_examples2.svg/759px-Correlation_examples2.svg.png)

几组(x, y)的点集，以及各个点集中x和y之间的相关系数。我们可以发现相关系数反映的是变量线性关系的噪声和相关性的方向（第一排），而不是相关性的斜率（中间），也不是各种非线性关系（第三排）。请注意：中间的图中斜率为0，但相关系数是没有意义的，因为此时变量Y是0。



## 更多内容
更多内容，比如全期望理论，联合分布律等，没有新的直觉需要建立，这里不再介绍，自己看书即可。


